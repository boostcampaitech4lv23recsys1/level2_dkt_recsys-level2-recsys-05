{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4257c315-e4b3-4e7c-bc69-4a38ec128dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.models import LightGCN\n",
    "\n",
    "import math\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b0d75b-8bd8-44aa-aa30-ef19c879172c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dcea6c2-de12-4bc2-9f3a-919965f809ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>assessmentItemID</th>\n",
       "      <th>testId</th>\n",
       "      <th>answerCode</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>KnowledgeTag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A060001001</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:11</td>\n",
       "      <td>7224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A060001002</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:14</td>\n",
       "      <td>7225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A060001003</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:22</td>\n",
       "      <td>7225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A060001004</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:29</td>\n",
       "      <td>7225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A060001005</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:36</td>\n",
       "      <td>7225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID assessmentItemID      testId  answerCode            Timestamp  \\\n",
       "0       0       A060001001  A060000001           1  2020-03-24 00:17:11   \n",
       "1       0       A060001002  A060000001           1  2020-03-24 00:17:14   \n",
       "2       0       A060001003  A060000001           1  2020-03-24 00:17:22   \n",
       "3       0       A060001004  A060000001           1  2020-03-24 00:17:29   \n",
       "4       0       A060001005  A060000001           1  2020-03-24 00:17:36   \n",
       "\n",
       "   KnowledgeTag  \n",
       "0          7224  \n",
       "1          7225  \n",
       "2          7225  \n",
       "3          7225  \n",
       "4          7225  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f131be-0c83-4499-b99f-691d72c60b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2526700, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat = pd.concat([train, test]).reset_index(drop=True)\n",
    "concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a671b11-8fe4-43d6-8e05-6d18a3310d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>assessmentItemID</th>\n",
       "      <th>testId</th>\n",
       "      <th>answerCode</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>KnowledgeTag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12796</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:11</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12797</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:14</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>12798</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:22</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12799</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:29</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>12800</td>\n",
       "      <td>A060000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:17:36</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  assessmentItemID      testId  answerCode            Timestamp  \\\n",
       "0       1             12796  A060000001           1  2020-03-24 00:17:11   \n",
       "1       1             12797  A060000001           1  2020-03-24 00:17:14   \n",
       "2       1             12798  A060000001           1  2020-03-24 00:17:22   \n",
       "3       1             12799  A060000001           1  2020-03-24 00:17:29   \n",
       "4       1             12800  A060000001           1  2020-03-24 00:17:36   \n",
       "\n",
       "   KnowledgeTag  \n",
       "0           556  \n",
       "1           557  \n",
       "2           557  \n",
       "3           557  \n",
       "4           557  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user2vec = {v:k+1 for k, v in enumerate(sorted(concat.userID.unique()))}\n",
    "n_user = concat.userID.nunique()\n",
    "\n",
    "item2vec = {v:k+n_user for k, v in enumerate(sorted(concat.assessmentItemID.unique()))}\n",
    "tag2vec = {v:k for k, v in enumerate(sorted(concat.KnowledgeTag.unique()))}\n",
    "\n",
    "train['userID'] = train['userID'].apply(lambda x : user2vec[x])\n",
    "test['userID'] = test['userID'].apply(lambda x : user2vec[x])\n",
    "concat['userID'] = concat['userID'].apply(lambda x : user2vec[x])\n",
    "train['assessmentItemID'] = train['assessmentItemID'].apply(lambda x : item2vec[x])\n",
    "test['assessmentItemID'] = test['assessmentItemID'].apply(lambda x : item2vec[x])\n",
    "concat['assessmentItemID'] = concat['assessmentItemID'].apply(lambda x : item2vec[x])\n",
    "train['KnowledgeTag'] = train['KnowledgeTag'].apply(lambda x : tag2vec[x])\n",
    "test['KnowledgeTag'] = test['KnowledgeTag'].apply(lambda x : tag2vec[x])\n",
    "concat['KnowledgeTag'] = concat['KnowledgeTag'].apply(lambda x : tag2vec[x])\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea25753-d399-48c5-ad94-7966e57731cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = concat[concat.answerCode >= 0]\n",
    "\n",
    "train_index, valid_index = train_test_split(data.index, random_state=0, test_size=0.2)\n",
    "\n",
    "train = data.loc[train_index, ['userID', 'assessmentItemID', 'KnowledgeTag', 'answerCode']]\n",
    "userid, itemid, tags, answer = train.userID, train.assessmentItemID, train.KnowledgeTag, train.answerCode\n",
    "train_edge, train_feature, train_label = [], [], []\n",
    "for user, item, tag, acode in zip(userid, itemid, tags, answer):\n",
    "    train_edge.append([user, item])\n",
    "    train_feature.append(tag)\n",
    "    train_label.append(acode)\n",
    "\n",
    "edge_train = torch.LongTensor(train_edge).T\n",
    "feature_train = torch.LongTensor(train_feature)\n",
    "y_train = torch.LongTensor(train_label)\n",
    "\n",
    "valid = data.loc[valid_index, ['userID', 'assessmentItemID', 'KnowledgeTag', 'answerCode']]\n",
    "userid, itemid, tags, answer = valid.userID, valid.assessmentItemID, valid.KnowledgeTag, valid.answerCode\n",
    "valid_edge, valid_feature, valid_label = [], [], []\n",
    "for user, item, tag, acode in zip(userid, itemid, tags, answer):\n",
    "    valid_edge.append([user, item])\n",
    "    valid_feature.append(tag)\n",
    "    valid_label.append(acode)\n",
    "    \n",
    "edge_valid = torch.LongTensor(valid_edge).T\n",
    "feature_valid = torch.LongTensor(valid_feature)\n",
    "y_valid = torch.LongTensor(valid_label)\n",
    "\n",
    "train_data = dict(edge=edge_train.to(device), feature=feature_train.to(device), label=y_train.to(device))\n",
    "valid_data = dict(edge=edge_valid.to(device), feature=feature_valid.to(device), label=y_valid.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f43b835-31fb-441d-b3be-2ac73b808548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "from torch_geometric.nn.conv import LGConv\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "\n",
    "\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "from torch_geometric.nn.conv import LGConv\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "\n",
    "\n",
    "class LightGCN(torch.nn.Module):\n",
    "    r\"\"\"The LightGCN model from the `\"LightGCN: Simplifying and Powering\n",
    "    Graph Convolution Network for Recommendation\"\n",
    "    <https://arxiv.org/abs/2002.02126>`_ paper.\n",
    "\n",
    "    :class:`~torch_geometric.nn.models.LightGCN` learns embeddings by linearly\n",
    "    propagating them on the underlying graph, and uses the weighted sum of the\n",
    "    embeddings learned at all layers as the final embedding\n",
    "\n",
    "    .. math::\n",
    "        \\textbf{x}_i = \\sum_{l=0}^{L} \\alpha_l \\textbf{x}^{(l)}_i,\n",
    "\n",
    "    where each layer's embedding is computed as\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{(l+1)}_i = \\sum_{j \\in \\mathcal{N}(i)}\n",
    "        \\frac{1}{\\sqrt{\\deg(i)\\deg(j)}}\\mathbf{x}^{(l)}_j.\n",
    "\n",
    "    Two prediction heads and trainign objectives are provided:\n",
    "    **link prediction** (via\n",
    "    :meth:`~torch_geometric.nn.models.LightGCN.link_pred_loss` and\n",
    "    :meth:`~torch_geometric.nn.models.LightGCN.predict_link`) and\n",
    "    **recommendation** (via\n",
    "    :meth:`~torch_geometric.nn.models.LightGCN.recommendation_loss` and\n",
    "    :meth:`~torch_geometric.nn.models.LightGCN.recommend`).\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        Embeddings are propagated according to the graph connectivity specified\n",
    "        by :obj:`edge_index` while rankings or link probabilities are computed\n",
    "        according to the edges specified by :obj:`edge_label_index`.\n",
    "\n",
    "    Args:\n",
    "        num_nodes (int): The number of nodes in the graph.\n",
    "        embedding_dim (int): The dimensionality of node embeddings.\n",
    "        num_layers (int): The number of\n",
    "            :class:`~torch_geometric.nn.conv.LGConv` layers.\n",
    "        alpha (float or Tensor, optional): The scalar or vector specifying the\n",
    "            re-weighting coefficients for aggregating the final embedding.\n",
    "            If set to :obj:`None`, the uniform initialization of\n",
    "            :obj:`1 / (num_layers + 1)` is used. (default: :obj:`None`)\n",
    "        **kwargs (optional): Additional arguments of the underlying\n",
    "            :class:`~torch_geometric.nn.conv.LGConv` layers.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        feature_len: int,\n",
    "        num_layers: int,\n",
    "        alpha: Optional[Union[float, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.feature_len = feature_len\n",
    "        \n",
    "        if alpha is None:\n",
    "            alpha = 1. / (num_layers + 1)\n",
    "\n",
    "        if isinstance(alpha, Tensor):\n",
    "            assert alpha.size(0) == num_layers + 1\n",
    "        else:\n",
    "            alpha = torch.tensor([alpha] * (num_layers + 1))\n",
    "        self.register_buffer('alpha', alpha)\n",
    "\n",
    "        self.embedding = Embedding(self.num_nodes, self.embedding_dim)\n",
    "        self.convs = ModuleList([LGConv(**kwargs) for _ in range(self.num_layers)])\n",
    "        self.embedding_feature = Embedding(self.feature_len, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim*2, self.hidden_dim)\n",
    "        self.ln = nn.Linear(self.hidden_dim, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "\n",
    "    def get_embedding(self, edge_index: Adj) -> Tensor:\n",
    "        x = self.embedding.weight\n",
    "        out = x * self.alpha[0]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            out = out + x * self.alpha[i + 1]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, edge_index: Adj, feature,\n",
    "                edge_label_index: OptTensor = None) -> Tensor:\n",
    "        r\"\"\"Computes rankings for pairs of nodes.\n",
    "\n",
    "        Args:\n",
    "            edge_index (Tensor or SparseTensor): Edge tensor specifying the\n",
    "                connectivity of the graph.\n",
    "            edge_label_index (Tensor, optional): Edge tensor specifying the\n",
    "                node pairs for which to compute rankings or probabilities.\n",
    "                If :obj:`edge_label_index` is set to :obj:`None`, all edges in\n",
    "                :obj:`edge_index` will be used instead. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        if edge_label_index is None:\n",
    "            if isinstance(edge_index, SparseTensor):\n",
    "                edge_label_index = torch.stack(edge_index.coo()[:2], dim=0)\n",
    "            else:\n",
    "                edge_label_index = edge_index\n",
    "\n",
    "        out = self.get_embedding(edge_index)\n",
    "        out_src = out[edge_label_index[0]]\n",
    "        out_dst = out[edge_label_index[1]]\n",
    "        \n",
    "        feature = self.embedding_feature(feature)\n",
    "        out = torch.concat([(out_src * out_dst), feature], 1)\n",
    "        \n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.ln(out)\n",
    "        return out.view(-1)\n",
    "\n",
    "    def predict_link(self, edge_index: Adj, edge_label_index: OptTensor = None,\n",
    "                     prob: bool = False) -> Tensor:\n",
    "        r\"\"\"Predict links between nodes specified in :obj:`edge_label_index`.\n",
    "\n",
    "        Args:\n",
    "            prob (bool): Whether probabilities should be returned. (default:\n",
    "                :obj:`False`)\n",
    "        \"\"\"\n",
    "        pred = self(edge_index, edge_label_index).sigmoid()\n",
    "        return pred if prob else pred.round()\n",
    "\n",
    "\n",
    "    def recommend(self, edge_index: Adj, src_index: OptTensor = None,\n",
    "                  dst_index: OptTensor = None, k: int = 1) -> Tensor:\n",
    "        r\"\"\"Get top-:math:`k` recommendations for nodes in :obj:`src_index`.\n",
    "\n",
    "        Args:\n",
    "            src_index (Tensor, optional): Node indices for which\n",
    "                recommendations should be generated.\n",
    "                If set to :obj:`None`, all nodes will be used.\n",
    "                (default: :obj:`None`)\n",
    "            dst_index (Tensor, optional): Node indices which represent the\n",
    "                possible recommendation choices.\n",
    "                If set to :obj:`None`, all nodes will be used.\n",
    "                (default: :obj:`None`)\n",
    "            k (int, optional): Number of recommendations. (default: :obj:`1`)\n",
    "        \"\"\"\n",
    "        out_src = out_dst = self.get_embedding(edge_index)\n",
    "\n",
    "        if src_index is not None:\n",
    "            out_src = out_src[src_index]\n",
    "\n",
    "        if dst_index is not None:\n",
    "            out_dst = out_dst[dst_index]\n",
    "\n",
    "        pred = out_src @ out_dst.t()\n",
    "        top_index = pred.topk(k, dim=-1).indices\n",
    "\n",
    "        if dst_index is not None:  # Map local top-indices to original indices.\n",
    "            top_index = dst_index[top_index.view(-1)].view(*top_index.size())\n",
    "\n",
    "        return top_index\n",
    "\n",
    "\n",
    "    def link_pred_loss(self, pred: Tensor, edge_label: Tensor,\n",
    "                       **kwargs) -> Tensor:\n",
    "        r\"\"\"Computes the model loss for a link prediction objective via the\n",
    "        :class:`torch.nn.BCEWithLogitsLoss`.\n",
    "\n",
    "        Args:\n",
    "            pred (Tensor): The predictions.\n",
    "            edge_label (Tensor): The ground-truth edge labels.\n",
    "            **kwargs (optional): Additional arguments of the underlying\n",
    "                :class:`torch.nn.BCEWithLogitsLoss` loss function.\n",
    "        \"\"\"\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss(**kwargs)\n",
    "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
    "\n",
    "\n",
    "    def recommendation_loss(self, pos_edge_rank: Tensor, neg_edge_rank: Tensor,\n",
    "                            lambda_reg: float = 1e-4, **kwargs) -> Tensor:\n",
    "        r\"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
    "        Personalized Ranking (BPR) loss.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            The i-th entry in the :obj:`pos_edge_rank` vector and i-th entry\n",
    "            in the :obj:`neg_edge_rank` entry must correspond to ranks of\n",
    "            positive and negative edges of the same entity (*e.g.*, user).\n",
    "\n",
    "        Args:\n",
    "            pos_edge_rank (Tensor): Positive edge rankings.\n",
    "            neg_edge_rank (Tensor): Negative edge rankings.\n",
    "            lambda_reg (int, optional): The :math:`L_2` regularization strength\n",
    "                of the Bayesian Personalized Ranking (BPR) loss.\n",
    "                (default: 1e-4)\n",
    "            **kwargs (optional): Additional arguments of the underlying\n",
    "                :class:`torch_geometric.nn.models.lightgcn.BPRLoss` loss\n",
    "                function.\n",
    "        \"\"\"\n",
    "        loss_fn = BPRLoss(lambda_reg, **kwargs)\n",
    "        return loss_fn(pos_edge_rank, neg_edge_rank, self.embedding.weight)\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.num_nodes}, '\n",
    "                f'{self.embedding_dim}, num_layers={self.num_layers})')\n",
    "\n",
    "\n",
    "\n",
    "class BPRLoss(_Loss):\n",
    "    r\"\"\"The Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "    The BPR loss is a pairwise loss that encourages the prediction of an\n",
    "    observed entry to be higher than its unobserved counterparts\n",
    "    (see `here <https://arxiv.org/abs/2002.02126>`__).\n",
    "\n",
    "    .. math::\n",
    "        L_{\\text{BPR}} = - \\sum_{u=1}^{M} \\sum_{i \\in \\mathcal{N}_u}\n",
    "        \\sum_{j \\not\\in \\mathcal{N}_u} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})\n",
    "        + \\lambda \\vert\\vert \\textbf{x}^{(0)} \\vert\\vert^2\n",
    "\n",
    "    where :math:`lambda` controls the :math:`L_2` regularization strength.\n",
    "    We compute the mean BPR loss for simplicity.\n",
    "\n",
    "    Args:\n",
    "        lambda_reg (float, optional): The :math:`L_2` regularization strength\n",
    "            (default: 0).\n",
    "        **kwargs (optional): Additional arguments of the underlying\n",
    "            :class:`torch.nn.modules.loss._Loss` class.\n",
    "    \"\"\"\n",
    "    __constants__ = ['lambda_reg']\n",
    "    lambda_reg: float\n",
    "\n",
    "    def __init__(self, lambda_reg: float = 0, **kwargs) -> None:\n",
    "        super().__init__(None, None, \"sum\", **kwargs)\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, positives: Tensor, negatives: Tensor,\n",
    "                parameters: Tensor = None) -> Tensor:\n",
    "        r\"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            The i-th entry in the :obj:`positives` vector and i-th entry\n",
    "            in the :obj:`negatives` entry should correspond to the same\n",
    "            entity (*.e.g*, user), as the BPR is a personalized ranking loss.\n",
    "\n",
    "        Args:\n",
    "            positives (Tensor): The vector of positive-pair rankings.\n",
    "            negatives (Tensor): The vector of negative-pair rankings.\n",
    "            parameters (Tensor, optional): The tensor of parameters which\n",
    "                should be used for :math:`L_2` regularization\n",
    "                (default: :obj:`None`).\n",
    "        \"\"\"\n",
    "        n_pairs = positives.size(0)\n",
    "        log_prob = F.logsigmoid(positives - negatives).mean()\n",
    "        regularization = 0\n",
    "\n",
    "        if self.lambda_reg != 0:\n",
    "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
    "\n",
    "        return (-log_prob + regularization) / n_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "338e3bed-096d-4684-bd9c-1c3fb32e2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data['label'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        edge = torch.stack([self.data['edge'][0][idx], self.data['edge'][1][idx]])\n",
    "        feature = self.data['feature'][idx]\n",
    "        label = self.data['label'][idx]\n",
    "        return edge, feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e7b963-31e8-4cac-9889-814dbbbbbb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = CustomDataset(train_data)\n",
    "train_dataloader = DataLoader(train_set, batch_size=2048, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7e5de91-3fa7-4888-9d4c-fb48c58c2694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987/987 [02:26<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0001, loss=0.607, acc=0.668, AUC=0.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/987 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0001, val_loss=0.604, val_acc=0.669, val_AUC=0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987/987 [02:25<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0002, loss=0.601, acc=0.675, AUC=0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/987 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0002, val_loss=0.586, val_acc=0.698, val_AUC=0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987/987 [02:26<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0003, loss=0.512, acc=0.751, AUC=0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/987 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0003, val_loss=0.488, val_acc=0.767, val_AUC=0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987/987 [02:26<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0004, loss=0.432, acc=0.800, AUC=0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/987 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0004, val_loss=0.484, val_acc=0.772, val_AUC=0.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987/987 [02:26<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0005, loss=0.321, acc=0.860, AUC=0.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/987 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0005, val_loss=0.555, val_acc=0.761, val_AUC=0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987/987 [02:26<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0006, loss=0.177, acc=0.930, AUC=0.979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/987 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0006, val_loss=0.744, val_acc=0.749, val_AUC=0.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987/987 [02:25<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * In epoch 0007, loss=0.087, acc=0.970, AUC=0.995\n",
      "best_acc=0.772, best_AUC=0.831\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "embedding = 256\n",
    "hidden_dim = 512\n",
    "layers = 2\n",
    "\n",
    "valid_edge = torch.stack([i for i in valid_data['edge']])\n",
    "valid_feature = valid_data['feature']\n",
    "valid_label = valid_data['label']\n",
    "feature = max(concat.KnowledgeTag.values)+1\n",
    "\n",
    "model = LightGCN(concat.userID.nunique()+concat.assessmentItemID.nunique(), embedding, hidden_dim, feature, layers)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "best_auc = 0\n",
    "best_acc = 0\n",
    "patience = 0\n",
    "for epoch in range(1001):\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_auc = 0.0\n",
    "    model.train()\n",
    "    for edge, feature, label in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "        preds = model(edge.T, feature)\n",
    "        loss = model.link_pred_loss(preds, label)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prob = preds.sigmoid()\n",
    "        prob = prob.detach().cpu().numpy()\n",
    "        acc = accuracy_score(label.detach().cpu().numpy(), prob > 0.5)\n",
    "        auc = roc_auc_score(label.detach().cpu().numpy(), prob)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        total_auc += auc\n",
    "        \n",
    "    if not epoch % 1:\n",
    "        print(f\" * In epoch {(epoch+1):04}, loss={total_loss/len(train_dataloader):.03f}, acc={total_acc/len(train_dataloader):.03f}, AUC={total_auc/len(train_dataloader):.03f}\")    \n",
    "    \n",
    "    with torch.no_grad() :\n",
    "        model.eval()\n",
    "        preds = model(valid_edge, valid_feature)\n",
    "        loss = model.link_pred_loss(preds, valid_label)\n",
    "\n",
    "        prob = preds.sigmoid()\n",
    "        prob = prob.detach().cpu().numpy()\n",
    "        acc = accuracy_score(valid_label.detach().cpu().numpy(), prob > 0.5)\n",
    "        auc = roc_auc_score(valid_label.detach().cpu().numpy(), prob)\n",
    "        \n",
    "    if auc > best_auc :\n",
    "        best_auc = auc\n",
    "        best_acc = acc\n",
    "        torch.save(\n",
    "            {\n",
    "                \"state_dict\": model.state_dict()\n",
    "            },\n",
    "            \"model_2.pt\",\n",
    "        )\n",
    "        patience = 0\n",
    "    else :\n",
    "        patience += 1\n",
    "        \n",
    "    if patience == 3 :\n",
    "        break\n",
    "        \n",
    "    if not epoch % 1:\n",
    "        print(f\" * In epoch {(epoch+1):04}, val_loss={loss:.03f}, val_acc={acc:.03f}, val_AUC={auc:.03f}\")\n",
    "        \n",
    "print(f\"best_acc={best_acc:.03f}, best_AUC={best_auc:.03f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "108b59d9-e06a-4c51-bf27-96b7c2708699",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    4,     5,    14,  ...,  7417,  7418,  7440],\n",
       "        [12407, 15190, 14926,  ..., 12795, 12795, 11174]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = concat[concat.answerCode == -1]\n",
    "users, features, items = [], [], []\n",
    "for _, (user, item, tag) in test[['userID', 'assessmentItemID', 'KnowledgeTag']].iterrows() :\n",
    "    users.append(user)\n",
    "    items.append(item)\n",
    "    features.append(tag)\n",
    "users = torch.LongTensor(users)\n",
    "items = torch.LongTensor(items)\n",
    "features = torch.LongTensor(features)\n",
    "test_edge = torch.stack([users, items])\n",
    "test_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e435ebb3-7715-40a8-94b0-ad773546da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = max(data.KnowledgeTag.values)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8b0def4-fbb3-4ec2-a10a-a23bef08eaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2905, 0.6706, 0.3809, 0.6843, 0.1597, 0.8890, 0.3735, 0.2440, 0.0682,\n",
       "        0.9097, 0.5829, 0.3120, 0.9815, 0.2285, 0.5526, 0.9674, 0.0602, 0.6482,\n",
       "        0.9040, 0.0627, 0.9611, 0.4802, 0.4992, 0.4836, 0.2932, 0.8730, 0.9356,\n",
       "        0.8392, 0.5730, 0.7386, 0.8369, 0.8767, 0.9242, 0.3176, 0.9703, 0.8153,\n",
       "        0.1886, 0.3730, 0.2812, 0.3085, 0.6109, 0.1006, 0.1279, 0.1599, 0.3782,\n",
       "        0.8856, 0.3330, 0.2337, 0.9865, 0.8707, 0.3951, 0.4807, 0.8490, 0.0713,\n",
       "        0.1354, 0.8612, 0.3778, 0.9026, 0.0641, 0.2295, 0.9163, 0.9140, 0.7971,\n",
       "        0.2752, 0.1240, 0.3064, 0.6886, 0.4206, 0.1543, 0.1878, 0.6296, 0.8736,\n",
       "        0.0423, 0.1443, 0.1656, 0.4593, 0.1129, 0.8866, 0.5015, 0.9003, 0.4466,\n",
       "        0.4766, 0.2121, 0.4335, 0.6643, 0.4071, 0.3626, 0.3372, 0.2966, 0.1143,\n",
       "        0.3527, 0.0314, 0.1771, 0.7526, 0.9534, 0.7454, 0.0267, 0.5991, 0.9624,\n",
       "        0.9016, 0.3473, 0.0726, 0.5335, 0.1426, 0.3810, 0.6283, 0.7870, 0.8780,\n",
       "        0.1149, 0.1987, 0.9290, 0.8416, 0.1098, 0.2159, 0.0689, 0.5593, 0.9412,\n",
       "        0.3437, 0.2692, 0.1793, 0.5996, 0.8708, 0.9945, 0.0258, 0.0246, 0.3792,\n",
       "        0.7858, 0.3017, 0.8716, 0.5732, 0.9271, 0.1968, 0.9433, 0.2272, 0.5641,\n",
       "        0.5696, 0.1223, 0.4420, 0.3692, 0.8556, 0.5771, 0.4546, 0.3130, 0.4720,\n",
       "        0.5681, 0.1663, 0.4129, 0.9823, 0.2696, 0.5960, 0.1496, 0.3542, 0.7173,\n",
       "        0.0149, 0.1504, 0.4330, 0.0754, 0.1677, 0.0464, 0.2720, 0.5360, 0.0446,\n",
       "        0.1793, 0.4786, 0.6552, 0.6180, 0.3089, 0.0355, 0.7376, 0.3743, 0.8302,\n",
       "        0.2306, 0.2955, 0.5533, 0.2451, 0.9545, 0.7376, 0.5930, 0.0728, 0.4032,\n",
       "        0.2720, 0.1732, 0.3980, 0.4621, 0.8843, 0.9858, 0.3925, 0.5224, 0.9248,\n",
       "        0.0277, 0.2313, 0.1427, 0.8311, 0.9931, 0.6504, 0.4845, 0.7842, 0.8697,\n",
       "        0.7266, 0.2210, 0.4706, 0.3421, 0.1618, 0.5470, 0.1805, 0.8688, 0.0423,\n",
       "        0.2434, 0.4298, 0.0568, 0.6551, 0.8059, 0.1085, 0.1727, 0.2027, 0.4106,\n",
       "        0.6824, 0.6101, 0.5237, 0.8403, 0.9375, 0.7190, 0.3452, 0.9812, 0.9179,\n",
       "        0.4355, 0.3188, 0.0690, 0.1958, 0.1718, 0.2120, 0.3606, 0.4835, 0.5449,\n",
       "        0.6914, 0.7013, 0.2944, 0.0511, 0.3977, 0.8880, 0.5311, 0.2324, 0.3295,\n",
       "        0.7714, 0.9607, 0.3521, 0.2496, 0.6592, 0.0616, 0.0956, 0.4906, 0.8316,\n",
       "        0.3493, 0.8957, 0.3826, 0.1075, 0.4526, 0.2116, 0.2006, 0.4231, 0.0259,\n",
       "        0.5476, 0.4440, 0.0359, 0.0175, 0.3885, 0.8065, 0.9393, 0.3739, 0.3836,\n",
       "        0.2525, 0.1711, 0.2390, 0.7974, 0.8833, 0.0953, 0.9211, 0.1058, 0.3323,\n",
       "        0.9980, 0.3825, 0.3506, 0.6520, 0.9161, 0.5384, 0.0950, 0.1403, 0.1980,\n",
       "        0.8595, 0.6206, 0.2169, 0.3754, 0.4203, 0.1692, 0.5337, 0.1745, 0.2044,\n",
       "        0.1960, 0.4360, 0.3583, 0.5650, 0.3771, 0.2187, 0.8786, 0.5068, 0.0235,\n",
       "        0.5170, 0.8983, 0.1343, 0.7244, 0.1448, 0.2438, 0.9534, 0.4375, 0.1584,\n",
       "        0.8231, 0.9928, 0.2306, 0.1667, 0.6970, 0.2004, 0.7909, 0.8455, 0.0169,\n",
       "        0.3627, 0.1447, 0.9230, 0.0197, 0.3599, 0.7368, 0.9408, 0.1231, 0.1893,\n",
       "        0.2308, 0.9041, 0.3013, 0.0504, 0.4824, 0.9118, 0.8390, 0.6856, 0.1805,\n",
       "        0.4920, 0.3404, 0.1339, 0.8758, 0.6108, 0.2264, 0.5098, 0.7573, 0.6259,\n",
       "        0.1967, 0.5228, 0.2149, 0.6187, 0.8261, 0.7649, 0.4185, 0.4844, 0.3685,\n",
       "        0.5009, 0.3341, 0.1533, 0.5311, 0.5649, 0.6080, 0.7461, 0.2382, 0.0667,\n",
       "        0.1794, 0.7042, 0.9675, 0.8750, 0.9438, 0.2462, 0.5725, 0.2351, 0.3865,\n",
       "        0.7964, 0.3864, 0.1578, 0.7806, 0.6327, 0.5071, 0.2473, 0.2845, 0.4199,\n",
       "        0.7716, 0.9794, 0.8092, 0.1794, 0.1326, 0.2313, 0.1956, 0.9921, 0.0930,\n",
       "        0.9303, 0.8677, 0.1050, 0.1296, 0.8592, 0.9777, 0.5163, 0.2416, 0.6294,\n",
       "        0.2468, 0.9029, 0.4824, 0.1651, 0.2801, 0.1597, 0.0099, 0.3558, 0.6891,\n",
       "        0.2279, 0.4025, 0.9746, 0.6456, 0.8771, 0.3675, 0.8660, 0.3237, 0.3607,\n",
       "        0.2727, 0.5473, 0.0488, 0.8007, 0.3338, 0.6677, 0.6628, 0.5857, 0.3370,\n",
       "        0.7840, 0.8877, 0.6962, 0.7620, 0.3139, 0.2244, 0.8619, 0.0834, 0.4481,\n",
       "        0.0488, 0.1639, 0.7584, 0.2378, 0.8172, 0.1582, 0.6178, 0.1147, 0.4640,\n",
       "        0.7379, 0.6796, 0.1599, 0.3930, 0.5137, 0.2276, 0.4721, 0.5710, 0.6009,\n",
       "        0.3945, 0.2814, 0.9079, 0.4777, 0.6703, 0.3555, 0.4506, 0.4788, 0.4066,\n",
       "        0.1878, 0.8730, 0.0657, 0.3343, 0.8040, 0.4407, 0.1934, 0.6793, 0.0899,\n",
       "        0.2456, 0.8268, 0.5781, 0.5238, 0.4506, 0.2226, 0.2539, 0.8943, 0.1956,\n",
       "        0.3728, 0.0951, 0.0169, 0.0753, 0.0677, 0.3559, 0.6486, 0.1718, 0.4681,\n",
       "        0.1740, 0.1226, 0.1560, 0.1413, 0.6518, 0.5604, 0.4669, 0.2005, 0.4703,\n",
       "        0.4470, 0.5905, 0.5671, 0.9095, 0.4551, 0.1595, 0.4055, 0.1696, 0.5768,\n",
       "        0.8809, 0.3367, 0.2996, 0.9479, 0.3611, 0.3942, 0.1497, 0.4457, 0.8583,\n",
       "        0.6988, 0.5710, 0.7840, 0.7205, 0.4292, 0.0690, 0.5623, 0.4224, 0.5359,\n",
       "        0.2762, 0.1403, 0.4009, 0.2331, 0.2601, 0.8289, 0.2808, 0.1419, 0.2786,\n",
       "        0.9206, 0.7780, 0.9030, 0.5366, 0.1900, 0.9632, 0.7600, 0.9132, 0.6751,\n",
       "        0.6714, 0.8744, 0.3987, 0.6611, 0.6148, 0.3586, 0.2887, 0.4619, 0.3432,\n",
       "        0.7174, 0.5978, 0.5085, 0.7886, 0.3206, 0.1612, 0.8618, 0.6153, 0.0640,\n",
       "        0.9761, 0.5148, 0.6126, 0.1013, 0.3908, 0.0886, 0.4819, 0.5289, 0.3684,\n",
       "        0.1003, 0.2188, 0.2352, 0.8346, 0.6113, 0.9491, 0.9684, 0.5731, 0.4594,\n",
       "        0.4574, 0.6218, 0.0959, 0.3380, 0.5990, 0.0866, 0.5465, 0.4875, 0.7895,\n",
       "        0.7364, 0.1461, 0.9485, 0.5634, 0.6587, 0.7131, 0.7624, 0.4736, 0.9662,\n",
       "        0.1703, 0.5405, 0.2856, 0.1517, 0.8303, 0.1747, 0.8639, 0.1752, 0.1934,\n",
       "        0.9446, 0.0626, 0.3633, 0.5472, 0.9422, 0.0587, 0.2099, 0.2858, 0.3799,\n",
       "        0.1458, 0.3290, 0.3964, 0.3415, 0.0299, 0.1940, 0.5805, 0.0506, 0.7183,\n",
       "        0.1227, 0.4930, 0.6832, 0.3845, 0.7818, 0.2198, 0.8639, 0.3284, 0.2988,\n",
       "        0.5725, 0.4531, 0.4686, 0.2198, 0.2817, 0.2378, 0.2831, 0.0541, 0.7590,\n",
       "        0.4684, 0.3279, 0.2981, 0.7570, 0.9003, 0.8891, 0.5245, 0.4079, 0.4136,\n",
       "        0.8580, 0.2891, 0.2832, 0.1939, 0.8676, 0.5825, 0.2196, 0.9561, 0.5031,\n",
       "        0.5118, 0.4911, 0.4517, 0.5456, 0.4201, 0.1576, 0.6938, 0.2570, 0.2376,\n",
       "        0.9651, 0.1549, 0.8909, 0.6379, 0.5145, 0.4733, 0.6241, 0.5486, 0.5939,\n",
       "        0.7581, 0.5038, 0.7578, 0.5823, 0.6235, 0.6968, 0.3887, 0.4156, 0.3915,\n",
       "        0.5774, 0.3964, 0.0745, 0.1519, 0.0501, 0.7473, 0.8849, 0.5482, 0.8306,\n",
       "        0.7640, 0.7384, 0.5790, 0.1443, 0.0727, 0.4762, 0.2032, 0.3271, 0.7774,\n",
       "        0.3357, 0.5183, 0.0471, 0.2538, 0.4738, 0.8085, 0.2572, 0.3964, 0.4720,\n",
       "        0.3208, 0.5045, 0.5101, 0.4774, 0.5637, 0.7791, 0.3862, 0.1053, 0.4861,\n",
       "        0.2839, 0.8447, 0.4360, 0.7011, 0.5334, 0.7058, 0.5418, 0.8080, 0.4575,\n",
       "        0.3872, 0.2686, 0.5301, 0.9112, 0.6699, 0.3861], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = LightGCN(concat.userID.nunique()+concat.assessmentItemID.nunique(), embedding, hidden_dim, feature, layers)\n",
    "test_model = test_model.to(device)\n",
    "test_model.load_state_dict(torch.load('model_2.pt')['state_dict'])\n",
    "\n",
    "test_model.eval()\n",
    "test_edge = test_edge.to(device)\n",
    "features = features.to(device)\n",
    "prediction = test_model(test_edge, features).sigmoid()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc10ab74-eeb8-4022-a7f8-05a1dda46301",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['prediction'] = prediction.detach().cpu()\n",
    "sub.to_csv(f'{embedding}_{hidden_dim}_{layers}_{best_auc:.03f}_tag.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f405b4a-84de-4bcd-9c7b-3dde94ca6eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
