
Start Training: Epoch 1







  4%|‚ñç         | 971/23253 [00:16<06:20, 58.53it/s]
Traceback (most recent call last):
  File "train.py", line 31, in <module>
    main(args)
  File "train.py", line 25, in main
    trainer.run(args, train_data, valid_data, model)
  File "/opt/ml/input/code/dkt_v2/src/trainer.py", line 35, in run
    train_auc, train_acc, train_loss = train(
  File "/opt/ml/input/code/dkt_v2/src/trainer.py", line 91, in train
    update_params(loss, model, optimizer, scheduler, args)
  File "/opt/ml/input/code/dkt_v2/src/trainer.py", line 201, in update_params
    optimizer.step()
  File "/opt/conda/envs/dkt/lib/python3.8/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/opt/conda/envs/dkt/lib/python3.8/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/opt/conda/envs/dkt/lib/python3.8/site-packages/torch/optim/adam.py", line 234, in step
    adam(params_with_grad,
  File "/opt/conda/envs/dkt/lib/python3.8/site-packages/torch/optim/adam.py", line 300, in adam
    func(params,
  File "/opt/conda/envs/dkt/lib/python3.8/site-packages/torch/optim/adam.py", line 363, in _single_tensor_adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt